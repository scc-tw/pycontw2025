"""
Hypothesis Verification Framework for FFI Benchmarks

Systematic verification of performance hypotheses H1-H11 from plan.md using:
- Rigorous statistical analysis (Mann-Whitney U, Cliff's delta)
- Advanced profiling with Brendan Gregg's methodology
- Dispatch pattern benchmarking
- Performance crossover point analysis

This framework validates each hypothesis with statistical evidence and profiling data.
"""

import os
import sys
import time
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Callable, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from framework.timer import BenchmarkTimer, format_nanoseconds
from framework.statistics import create_statistical_analyzer, ComparisonResult
from framework.profiling import create_advanced_profiler, PerfProfile, CacheAnalysis
from framework.environment import create_environment_validator, EnvironmentStatus
from framework.function_verification import create_ffi_function_verifier
from benchmarks.dispatch_bench import create_dispatch_benchmark


class HypothesisStatus(Enum):
    """Status of hypothesis verification."""
    SUPPORTED = "supported"
    REJECTED = "rejected"
    INCONCLUSIVE = "inconclusive"
    ERROR = "error"


@dataclass
class HypothesisResult:
    """Results from hypothesis verification."""
    hypothesis_id: str
    description: str
    status: HypothesisStatus
    statistical_evidence: Dict[str, Any] = field(default_factory=dict)
    profiling_evidence: Dict[str, Any] = field(default_factory=dict)
    performance_data: Dict[str, Any] = field(default_factory=dict)
    conclusion: str = ""
    confidence_level: float = 0.0
    effect_size: float = 0.0
    practical_significance: bool = False


class HypothesisVerificationFramework:
    """Framework for systematic verification of FFI performance hypotheses."""
    
    def __init__(self, output_dir: Optional[str] = None):
        """Initialize the hypothesis verification framework."""
        self.output_dir = Path(output_dir) if output_dir else Path.cwd() / "hypothesis_results"
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize analysis tools
        self.timer = BenchmarkTimer(target_relative_error=0.02, max_time_seconds=60)
        self.stats_analyzer = create_statistical_analyzer(alpha=0.05, effect_size_threshold=0.2)
        self.profiler = create_advanced_profiler(str(self.output_dir))
        self.dispatch_benchmark = create_dispatch_benchmark(100)
        self.environment_validator = create_environment_validator(strict_mode=True)
        self.function_verifier = create_ffi_function_verifier()
        
        # Results storage
        self.results: Dict[str, HypothesisResult] = {}
        self.environment_report = None
        self.function_consistency_report = None
        
        print(f"üß™ Hypothesis Verification Framework initialized")
        print(f"üìÅ Results directory: {self.output_dir}")
    
    def verify_all_hypotheses(self) -> Dict[str, HypothesisResult]:
        """Verify all hypotheses H1-H11 systematically."""
        print("\nüöÄ Starting comprehensive hypothesis verification...")
        print("=" * 80)
        
        # CRITICAL: Validate environment before starting benchmarks
        print("\nüå°Ô∏è STEP 0: Environment Validation and Control")
        self.environment_report = self.environment_validator.validate_environment()
        self.environment_validator.print_environment_report(self.environment_report)
        
        # Check if environment is suitable for benchmarking
        if self.environment_report.status == EnvironmentStatus.CRITICAL:
            print("üö® CRITICAL: Environment unsuitable for benchmarking!")
            print("üìã Please address critical issues before proceeding:")
            for rec in self.environment_report.recommendations:
                print(f"  ‚Ä¢ {rec}")
            print("\nAborting hypothesis verification due to critical environment issues.")
            return self.results
        elif self.environment_report.status == EnvironmentStatus.SUBOPTIMAL:
            print("‚ö†Ô∏è WARNING: Suboptimal environment detected")
            print("üìã Consider addressing these issues for better results:")
            for rec in self.environment_report.recommendations[:3]:  # Show top 3
                print(f"  ‚Ä¢ {rec}")
            print("\nProceeding with verification (results may have higher variance)...")
        elif self.environment_report.status == EnvironmentStatus.ACCEPTABLE:
            print("‚úÖ Environment acceptable for benchmarking")
        else:  # OPTIMAL
            print("üéØ Environment optimal for benchmarking!")
        
        # Attempt environment optimization
        optimizations = self.environment_validator.optimize_environment()
        if optimizations['successful']:
            print(f"üîß Applied {len(optimizations['successful'])} optimizations")
        if optimizations['requires_sudo']:
            print(f"üí° {len(optimizations['requires_sudo'])} optimizations require sudo privileges")
        
        print(f"\n‚è±Ô∏è Starting benchmark validation with environment score: {self.environment_report.validation_score:.2f}")
        print("=" * 80)
        
        # CRITICAL: Verify identical C function calls across FFI methods
        print("\nüéØ STEP 1: Function Consistency Verification")
        self.function_consistency_report = self.function_verifier.verify_function_consistency()
        
        # Check if function consistency is acceptable for valid comparison
        if self.function_consistency_report.consistency_score < 0.7:
            print("üö® CRITICAL: Function consistency too low for valid comparison!")
            print("üìã FFI methods call different functions - experimental validity compromised")
            print(f"   Consistency score: {self.function_consistency_report.consistency_score:.1%} (minimum: 70%)")
            print("   This violates controlled experiment principles")
            print("\nAborting hypothesis verification due to contaminated experimental design.")
            return self.results
        elif self.function_consistency_report.consistency_score < 0.8:
            print("‚ö†Ô∏è WARNING: Suboptimal function consistency detected")
            print(f"   Consistency score: {self.function_consistency_report.consistency_score:.1%}")
            print("   Some FFI methods may not call identical C functions")
            print("   Results should be interpreted with caution")
        else:
            print(f"‚úÖ Excellent function consistency: {self.function_consistency_report.consistency_score:.1%}")
            print("   All FFI methods call identical C functions - experimental validity confirmed")
        
        # Generate function consistency report
        consistency_report_path = self.output_dir / "function_consistency_report.md"
        self.function_verifier.generate_consistency_report(
            self.function_consistency_report, 
            str(consistency_report_path)
        )
        
        print("=" * 80)
        
        # Core FFI Performance Hypotheses (H1-H5)
        self.verify_h1_cffi_faster_than_ctypes()
        self.verify_h2_call_overhead_dominates()
        self.verify_h3_type_conversion_overhead()
        self.verify_h4_memory_management_overhead()
        self.verify_h5_callback_performance_penalty()
        
        # Advanced Performance Analysis (H6-H8)
        self.verify_h6_zero_copy_benefits()
        self.verify_h7_crossover_point_analysis()
        self.verify_h8_gil_impact_on_ffi()
        
        # Dispatch Pattern Analysis (H9-H11)
        self.verify_h9_dict_vs_if_elif_scaling()
        self.verify_h10_cache_warming_effects()
        self.verify_h11_locality_effects()
        
        # Generate comprehensive report
        self._generate_final_report()
        
        return self.results
    
    def verify_h1_cffi_faster_than_ctypes(self):
        """H1: CFFI outperforms ctypes for equivalent operations (14-138% faster)."""
        print("\nüß™ H1: Verifying CFFI vs ctypes performance WITH PROFILING")
        
        try:
            # Import benchmark implementations
            from benchmarks.ctypes_bench import CtypesBenchmark
            from benchmarks.cffi_bench import CFfiBenchmark
            
            ctypes_bench = CtypesBenchmark()
            cffi_bench = CFfiBenchmark()
            
            # Test multiple operation types WITH PROFILING INTEGRATION
            operations = [
                ('integer_ops', lambda: ctypes_bench.integer_operations(), 
                               lambda: cffi_bench.integer_operations()),
                ('float_ops', lambda: ctypes_bench.float_operations_double(),
                             lambda: cffi_bench.float_operations_double()),
                ('array_readonly', lambda: ctypes_bench.array_operations_readonly(),
                                  lambda: cffi_bench.array_operations_readonly())
            ]
            
            results = {}
            for op_name, ctypes_func, cffi_func in operations:
                print(f"  üî¨ Testing {op_name} with hardware counters...")
                
                # FIXED: Integrate profiling as per review.md requirements
                implementations = {
                    'ctypes': ctypes_func,
                    'cffi': cffi_func
                }
                
                # Statistical comparison WITH profiling
                comparison = self.timer.statistical_comparison(implementations)
                
                # BRENDAN GREGG METHODOLOGY INTEGRATION (addressing review.md criticism)
                print(f"    üî• Generating flame graphs for {op_name}...")
                
                # Hardware counter analysis for ctypes (Brendan Gregg methodology)
                try:
                    ctypes_hw_profile = self.profiler.analyze_cache_performance(ctypes_func, 10000)
                    print(f"    üìä ctypes {op_name} - IPC: {ctypes_hw_profile.instructions_per_cycle:.2f}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Hardware profiling failed for ctypes: {e}")
                    ctypes_hw_profile = None
                
                # Hardware counter analysis for cffi (Brendan Gregg methodology)
                try:
                    cffi_hw_profile = self.profiler.analyze_cache_performance(cffi_func, 10000)
                    print(f"    üìä cffi {op_name} - IPC: {cffi_hw_profile.instructions_per_cycle:.2f}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Hardware profiling failed for cffi: {e}")
                    cffi_hw_profile = None
                
                # Flame graph generation (as required by review.md)
                try:
                    ctypes_flame = self.profiler.profile_cpu_with_flamegraph(
                        ctypes_func, duration_seconds=3
                    )
                    print(f"    üî• Generated ctypes flame graph: {ctypes_flame.flame_graph_path}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Flame graph generation failed for ctypes: {e}")
                    ctypes_flame = None
                
                try:
                    cffi_flame = self.profiler.profile_cpu_with_flamegraph(
                        cffi_func, duration_seconds=3  
                    )
                    print(f"    üî• Generated cffi flame graph: {cffi_flame.flame_graph_path}")
                except Exception as e:
                    print(f"    ‚ö†Ô∏è Flame graph generation failed for cffi: {e}")
                    cffi_flame = None
                
                # Cache analysis integration (results already captured above)
                if ctypes_hw_profile and cffi_hw_profile:
                    ctypes_cache_miss = (ctypes_hw_profile.cache_misses / ctypes_hw_profile.cache_references) if ctypes_hw_profile.cache_references > 0 else 0.0
                    cffi_cache_miss = (cffi_hw_profile.cache_misses / cffi_hw_profile.cache_references) if cffi_hw_profile.cache_references > 0 else 0.0
                    print(f"    üíæ Cache miss rates - ctypes: {ctypes_cache_miss:.3f}, cffi: {cffi_cache_miss:.3f}")
                    cache_analysis = {
                        'ctypes': {'cache_miss_rate': ctypes_cache_miss, 'l1_miss_rate': ctypes_hw_profile.l1_miss_rate},
                        'cffi': {'cache_miss_rate': cffi_cache_miss, 'l1_miss_rate': cffi_hw_profile.l1_miss_rate}
                    }
                else:
                    cache_analysis = {}
                
                results[op_name] = {
                    'comparison': comparison,
                    'ctypes_hw_profile': ctypes_hw_profile,
                    'cffi_hw_profile': cffi_hw_profile,
                    'ctypes_flame': ctypes_flame,
                    'cffi_flame': cffi_flame,
                    'cache_analysis': cache_analysis
                }
            
            # Determine overall result
            supported_operations = sum(1 for r in results.values() 
                                     if r['comparison']['hypothesis_supported'])
            
            status = HypothesisStatus.SUPPORTED if supported_operations >= 2 else HypothesisStatus.REJECTED
            
            # Calculate average speedup
            speedups = []
            for r in results.values():
                comp = r['comparison']['comparison']
                if comp.faster_method == 'cffi':
                    speedups.append(comp.relative_performance)
            
            avg_speedup = np.mean(speedups) if speedups else 0.0
            
            self.results['H1'] = HypothesisResult(
                hypothesis_id='H1',
                description='CFFI outperforms ctypes for equivalent operations',
                status=status,
                statistical_evidence=results,
                performance_data={'average_speedup': avg_speedup, 'operations_tested': len(operations)},
                conclusion=f"CFFI is {avg_speedup:.1f}x faster on average across {supported_operations}/{len(operations)} operations",
                confidence_level=0.95,
                effect_size=avg_speedup - 1.0,
                practical_significance=avg_speedup > 1.2
            )
            
        except Exception as e:
            self.results['H1'] = HypothesisResult(
                hypothesis_id='H1',
                description='CFFI outperforms ctypes for equivalent operations',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h2_call_overhead_dominates(self):
        """H2: Call overhead dominates for simple operations (<1000 cycles)."""
        print("\nüß™ H2: Verifying call overhead dominance")
        
        try:
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            bench = CtypesBenchmark()
            
            # Test operations of varying complexity
            operations = [
                ('noop', lambda: bench.lib.noop()),
                ('return_int', lambda: bench.lib.return_int()),
                ('add_small', lambda: bench.lib.add_int32(1, 2)),
                ('add_large', lambda: bench.lib.add_int64(1000000, 2000000))
            ]
            
            results = {}
            
            for op_name, op_func in operations:
                print(f"  Profiling {op_name}...")
                
                # Measure with cache analysis
                cache_analysis = self.profiler.analyze_cache_performance(op_func, 10000)
                
                # Measure timing
                timing_result = self.timer.measure_with_warmup(op_func)
                
                results[op_name] = {
                    'timing': timing_result,
                    'cache_analysis': cache_analysis,
                    'cycles_per_call': timing_result['wall_time']['median_ns'] * 2.5  # Assume 2.5 GHz
                }
            
            # Check if simple operations are dominated by overhead
            simple_ops = ['noop', 'return_int', 'add_small']
            overhead_dominated = all(
                results[op]['cycles_per_call'] < 1000 for op in simple_ops
            )
            
            status = HypothesisStatus.SUPPORTED if overhead_dominated else HypothesisStatus.REJECTED
            
            # Build conclusion string safely
            op_summaries = []
            for op in simple_ops:
                cycles = results[op]['cycles_per_call']
                op_summaries.append(f'{op}: {cycles:.0f} cycles')
            conclusion_text = f"Simple operations: {', '.join(op_summaries)}"
            
            self.results['H2'] = HypothesisResult(
                hypothesis_id='H2',
                description='Call overhead dominates for simple operations (<1000 cycles)',
                status=status,
                profiling_evidence=results,
                conclusion=conclusion_text,
                confidence_level=0.90,
                practical_significance=overhead_dominated
            )
            
        except Exception as e:
            self.results['H2'] = HypothesisResult(
                hypothesis_id='H2',
                description='Call overhead dominates for simple operations',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h3_type_conversion_overhead(self):
        """H3: Type conversion adds 20-50% overhead for complex types."""
        print("\nüß™ H3: Verifying type conversion overhead")
        
        try:
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            bench = CtypesBenchmark()
            
            # Compare simple vs complex type operations
            simple_ops = [
                ('int32', lambda: bench.lib.add_int32(1, 2)),
                ('int64', lambda: bench.lib.add_int64(1, 2))
            ]
            
            complex_ops = [
                ('float64', lambda: bench.lib.add_double(1.5, 2.5)),
                ('bool', lambda: bench.lib.logical_and(True, False))
            ]
            
            simple_times = []
            complex_times = []
            
            for op_name, op_func in simple_ops:
                timing = self.timer.measure_with_warmup(op_func)
                simple_times.append(timing['wall_time']['median_ns'])
                print(f"  {op_name}: {format_nanoseconds(timing['wall_time']['median_ns'])}")
            
            for op_name, op_func in complex_ops:
                timing = self.timer.measure_with_warmup(op_func)
                complex_times.append(timing['wall_time']['median_ns'])
                print(f"  {op_name}: {format_nanoseconds(timing['wall_time']['median_ns'])}")
            
            # Calculate overhead
            avg_simple = np.mean(simple_times)
            avg_complex = np.mean(complex_times)
            overhead_percent = ((avg_complex - avg_simple) / avg_simple) * 100
            
            # Statistical comparison
            comparison = self.stats_analyzer.mann_whitney_u_test(
                simple_times, complex_times, 'less'  # Test if simple < complex
            )
            
            status = HypothesisStatus.SUPPORTED if (
                comparison['significant'] and 20 <= overhead_percent <= 50
            ) else HypothesisStatus.REJECTED
            
            self.results['H3'] = HypothesisResult(
                hypothesis_id='H3',
                description='Type conversion adds 20-50% overhead for complex types',
                status=status,
                statistical_evidence=comparison,
                performance_data={
                    'simple_avg_ns': avg_simple,
                    'complex_avg_ns': avg_complex,
                    'overhead_percent': overhead_percent
                },
                conclusion=f"Type conversion overhead: {overhead_percent:.1f}%",
                confidence_level=1 - comparison['p_value'],
                effect_size=comparison['cliff_delta'],
                practical_significance=overhead_percent >= 20
            )
            
        except Exception as e:
            self.results['H3'] = HypothesisResult(
                hypothesis_id='H3',
                description='Type conversion adds overhead for complex types',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h4_memory_management_overhead(self):
        """H4: Memory allocation/deallocation adds significant overhead."""
        print("\nüß™ H4: Verifying memory management overhead")
        
        try:
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            bench = CtypesBenchmark()
            
            # Compare operations with/without memory allocation
            no_alloc_ops = [
                lambda: bench.lib.add_int32(1, 2),
                lambda: bench.lib.return_int()
            ]
            
            alloc_ops = [
                lambda: bench.lib.string_concat(b"hello", b"world"),
                lambda: bench.benchmark_struct_operations(10)
            ]
            
            # Measure baseline (no allocation)
            no_alloc_times = []
            for op in no_alloc_ops:
                timing = self.timer.measure_with_warmup(op)
                no_alloc_times.append(timing['wall_time']['median_ns'])
            
            # Measure with allocation
            alloc_times = []
            for op in alloc_ops:
                timing = self.timer.measure_with_warmup(op)
                alloc_times.append(timing['wall_time']['median_ns'])
            
            # Statistical comparison
            comparison = self.stats_analyzer.mann_whitney_u_test(
                no_alloc_times, alloc_times, 'less'  # Test if no_alloc < alloc
            )
            
            avg_no_alloc = np.mean(no_alloc_times)
            avg_alloc = np.mean(alloc_times)
            overhead_factor = avg_alloc / avg_no_alloc
            
            status = HypothesisStatus.SUPPORTED if (
                comparison['significant'] and overhead_factor > 2.0
            ) else HypothesisStatus.REJECTED
            
            self.results['H4'] = HypothesisResult(
                hypothesis_id='H4',
                description='Memory allocation/deallocation adds significant overhead',
                status=status,
                statistical_evidence=comparison,
                performance_data={
                    'no_alloc_avg_ns': avg_no_alloc,
                    'alloc_avg_ns': avg_alloc,
                    'overhead_factor': overhead_factor
                },
                conclusion=f"Memory allocation overhead: {overhead_factor:.1f}x slower",
                confidence_level=1 - comparison['p_value'],
                effect_size=comparison['cliff_delta'],
                practical_significance=overhead_factor > 1.5
            )
            
        except Exception as e:
            self.results['H4'] = HypothesisResult(
                hypothesis_id='H4',
                description='Memory allocation/deallocation adds significant overhead',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h5_callback_performance_penalty(self):
        """H5: Callbacks impose 3-10x performance penalty."""
        print("\nüß™ H5: Verifying callback performance penalty")
        
        try:
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            bench = CtypesBenchmark()
            
            # Direct C function call baseline
            direct_call = lambda: bench.lib.c_transform(42)
            
            # Python callback through FFI
            def python_transform(x):
                return x * 2
            
            callback_call = lambda: bench.lib.apply_callback(42, python_transform)
            
            # Measure both approaches
            direct_timing = self.timer.measure_with_warmup(direct_call)
            callback_timing = self.timer.measure_with_warmup(callback_call)
            
            direct_ns = direct_timing['wall_time']['median_ns']
            callback_ns = callback_timing['wall_time']['median_ns']
            penalty_factor = callback_ns / direct_ns
            
            # Statistical comparison
            comparison = self.timer.hypothesis_test(
                direct_call, callback_call, 'direct', 'callback', 'method1_faster'
            )
            
            status = HypothesisStatus.SUPPORTED if (
                comparison['hypothesis_supported'] and 3 <= penalty_factor <= 10
            ) else HypothesisStatus.REJECTED
            
            self.results['H5'] = HypothesisResult(
                hypothesis_id='H5',
                description='Callbacks impose 3-10x performance penalty',
                status=status,
                statistical_evidence=comparison,
                performance_data={
                    'direct_ns': direct_ns,
                    'callback_ns': callback_ns,
                    'penalty_factor': penalty_factor
                },
                conclusion=f"Callback penalty: {penalty_factor:.1f}x slower",
                confidence_level=0.95,
                effect_size=penalty_factor - 1.0,
                practical_significance=penalty_factor >= 3.0
            )
            
        except Exception as e:
            self.results['H5'] = HypothesisResult(
                hypothesis_id='H5',
                description='Callbacks impose performance penalty',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h6_zero_copy_benefits(self):
        """H6: Zero-copy operations provide significant benefits."""
        print("\nüß™ H6: Verifying zero-copy benefits")
        
        try:
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            bench = CtypesBenchmark()
            
            # Array size scaling test
            sizes = [100, 1000, 10000]
            results = {}
            
            for size in sizes:
                print(f"  Testing array size {size}...")
                
                # Zero-copy vs copy performance
                zero_copy_time = bench.benchmark_array_operations(size, zero_copy=True)
                copy_time = bench.benchmark_array_operations(size, zero_copy=False)
                
                benefit_factor = copy_time / zero_copy_time
                results[size] = {
                    'zero_copy_ns': zero_copy_time,
                    'copy_ns': copy_time,
                    'benefit_factor': benefit_factor
                }
            
            # Calculate average benefit
            avg_benefit = np.mean([r['benefit_factor'] for r in results.values()])
            
            # Benefits should increase with size
            benefit_scaling = all(
                results[sizes[i+1]]['benefit_factor'] >= results[sizes[i]]['benefit_factor']
                for i in range(len(sizes)-1)
            )
            
            status = HypothesisStatus.SUPPORTED if (
                avg_benefit > 1.5 and benefit_scaling
            ) else HypothesisStatus.REJECTED
            
            self.results['H6'] = HypothesisResult(
                hypothesis_id='H6',
                description='Zero-copy operations provide significant benefits',
                status=status,
                performance_data=results,
                conclusion=f"Average zero-copy benefit: {avg_benefit:.1f}x faster",
                confidence_level=0.90,
                effect_size=avg_benefit - 1.0,
                practical_significance=avg_benefit > 1.5
            )
            
        except Exception as e:
            self.results['H6'] = HypothesisResult(
                hypothesis_id='H6',
                description='Zero-copy operations provide significant benefits',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h7_crossover_point_analysis(self):
        """H7: Crossover point exists where FFI overhead becomes negligible."""
        print("\nüß™ H7: Verifying FFI overhead crossover point WITH ALL 4 FFI METHODS")
        
        try:
            # Import all FFI implementations
            from benchmarks.ctypes_bench import CtypesBenchmark
            from benchmarks.cffi_bench import CFfiBenchmark
            from benchmarks.pybind11_bench import Pybind11Benchmark
            from benchmarks.pyo3_bench import PyO3Benchmark
            
            # Initialize benchmarks
            ctypes_bench = CtypesBenchmark()
            cffi_bench = CFfiBenchmark()
            
            # Try to initialize pybind11 and PyO3 (may fail)
            try:
                pybind11_bench = Pybind11Benchmark()
                has_pybind11 = True
            except RuntimeError:
                pybind11_bench = None
                has_pybind11 = False
                print("  ‚ö†Ô∏è pybind11 not available for crossover analysis")
            
            try:
                pyo3_bench = PyO3Benchmark()
                has_pyo3 = True
            except RuntimeError:
                pyo3_bench = None
                has_pyo3 = False
                print("  ‚ö†Ô∏è PyO3 not available for crossover analysis")
            
            # Comprehensive scaling analysis: small to large operations
            operation_sizes = [
                ('vector_dot_1K', 1000, 'dot_product'),
                ('vector_dot_10K', 10000, 'dot_product'),
                ('matrix_10x10', 100, 'matrix_multiply'),  # 10x10 matrix = 100 elements
                ('matrix_50x50', 2500, 'matrix_multiply'),  # 50x50 matrix = 2500 elements
                ('matrix_100x100', 10000, 'matrix_multiply'),  # 100x100 matrix = 10K elements
                ('matrix_200x200', 40000, 'matrix_multiply'),  # 200x200 matrix = 40K elements
            ]
            
            results = {}
            crossover_points = {}
            
            for op_name, scale_factor, op_type in operation_sizes:
                print(f"  üî¨ Testing {op_name} (scale: {scale_factor})...")
                
                # Pure Python baseline
                if op_type == 'dot_product':
                    def python_baseline():
                        import numpy as np
                        a = np.random.random(scale_factor)
                        b = np.random.random(scale_factor)
                        return np.dot(a, b)
                else:  # matrix_multiply
                    matrix_size = int(scale_factor ** 0.5)
                    def python_baseline():
                        import numpy as np
                        a = np.random.random((matrix_size, matrix_size))
                        b = np.random.random((matrix_size, matrix_size))
                        return np.dot(a, b)
                
                # Measure Python baseline
                python_time = self.timer.measure_with_warmup(python_baseline)['wall_time']['median_ns']
                
                # Test each FFI method
                ffi_results = {'python': python_time}
                
                # ctypes
                if op_type == 'dot_product':
                    ctypes_time = self.timer.measure_with_warmup(
                        lambda: ctypes_bench.compute_operations_dot_product(scale_factor)
                    )['wall_time']['median_ns']
                else:
                    matrix_size = int(scale_factor ** 0.5)
                    ctypes_time = self.timer.measure_with_warmup(
                        lambda: ctypes_bench.compute_operations_matrix_multiply(matrix_size, matrix_size, matrix_size)
                    )['wall_time']['median_ns']
                ffi_results['ctypes'] = ctypes_time
                
                # cffi
                if op_type == 'dot_product':
                    cffi_time = self.timer.measure_with_warmup(
                        lambda: cffi_bench.compute_operations_dot_product(scale_factor)
                    )['wall_time']['median_ns']
                else:
                    matrix_size = int(scale_factor ** 0.5)
                    cffi_time = self.timer.measure_with_warmup(
                        lambda: cffi_bench.compute_operations_matrix_multiply(matrix_size, matrix_size, matrix_size)
                    )['wall_time']['median_ns']
                ffi_results['cffi'] = cffi_time
                
                # pybind11 (if available)
                if has_pybind11:
                    if op_type == 'dot_product':
                        pybind11_time = self.timer.measure_with_warmup(
                            lambda: pybind11_bench.matrix_operations_dot_product(scale_factor)
                        )['wall_time']['median_ns']
                    else:
                        matrix_size = int(scale_factor ** 0.5)
                        pybind11_time = self.timer.measure_with_warmup(
                            lambda: pybind11_bench.matrix_operations_multiply(matrix_size)
                        )['wall_time']['median_ns']
                    ffi_results['pybind11'] = pybind11_time
                
                # PyO3 (if available)  
                if has_pyo3:
                    if op_type == 'dot_product':
                        pyo3_time = self.timer.measure_with_warmup(
                            lambda: pyo3_bench.matrix_operations_dot_product(scale_factor)
                        )['wall_time']['median_ns']
                    else:
                        matrix_size = int(scale_factor ** 0.5)
                        pyo3_time = self.timer.measure_with_warmup(
                            lambda: pyo3_bench.benchmark_matrix_operations(matrix_size)
                        )['wall_time']['median_ns']
                    ffi_results['pyo3'] = pyo3_time
                
                # Calculate advantages for all FFI methods
                advantages = {}
                for method, time_ns in ffi_results.items():
                    if method != 'python':
                        advantages[method] = python_time / time_ns
                        print(f"    üìä {method}: {advantages[method]:.2f}x vs Python")
                
                results[op_name] = {
                    'scale_factor': scale_factor,
                    'operation_type': op_type,
                    'times': ffi_results,
                    'advantages': advantages
                }
            
            # Find crossover points for each FFI method
            for method in ['ctypes', 'cffi', 'pybind11', 'pyo3']:
                crossover_scale = None
                for op_name, data in results.items():
                    if method in data['advantages'] and data['advantages'][method] > 1.0:
                        crossover_scale = data['scale_factor']
                        break
                crossover_points[method] = crossover_scale
                
                if crossover_scale:
                    print(f"  üéØ {method} crossover: scale {crossover_scale}")
                else:
                    print(f"  ‚ùå {method}: no crossover found")
            
            # Determine overall hypothesis status
            methods_with_crossover = sum(1 for cp in crossover_points.values() if cp is not None)
            status = HypothesisStatus.SUPPORTED if methods_with_crossover >= 2 else HypothesisStatus.REJECTED
            
            # Calculate scaling efficiency
            scaling_analysis = {}
            for method in ['ctypes', 'cffi', 'pybind11', 'pyo3']:
                if method in results['vector_dot_1K']['advantages'] and method in results['matrix_200x200']['advantages']:
                    small_advantage = results['vector_dot_1K']['advantages'][method]
                    large_advantage = results['matrix_200x200']['advantages'][method]
                    scaling_efficiency = large_advantage / small_advantage if small_advantage > 0 else 0
                    scaling_analysis[method] = {
                        'small_scale_advantage': small_advantage,
                        'large_scale_advantage': large_advantage,
                        'scaling_efficiency': scaling_efficiency
                    }
            
            self.results['H7'] = HypothesisResult(
                hypothesis_id='H7',
                description='Crossover point exists where FFI overhead becomes negligible',
                status=status,
                performance_data={
                    'scaling_results': results,
                    'crossover_points': crossover_points,
                    'scaling_analysis': scaling_analysis
                },
                conclusion=f"Crossover analysis: {methods_with_crossover}/4 FFI methods show crossover points",
                confidence_level=0.90,
                effect_size=methods_with_crossover / 4.0,
                practical_significance=methods_with_crossover >= 2
            )
            
        except Exception as e:
            self.results['H7'] = HypothesisResult(
                hypothesis_id='H7',
                description='Crossover point analysis',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h8_gil_impact_on_ffi(self):
        """H8: GIL significantly impacts FFI performance in threaded scenarios."""
        print("\nüß™ H8: Verifying GIL impact on FFI performance")
        
        try:
            import sys
            import threading
            import concurrent.futures
            from benchmarks.ctypes_bench import CtypesBenchmark
            
            # Check if Python has GIL disabled
            gil_disabled = hasattr(sys, '_is_gil_enabled') and not sys._is_gil_enabled()
            print(f"  üîç GIL Status: {'DISABLED' if gil_disabled else 'ENABLED'}")
            
            if gil_disabled:
                print("  üöÄ Full GIL impact analysis with --disable-gil Python")
                status_suffix = " (--disable-gil Python)"
            else:
                print("  ‚ö†Ô∏è Limited GIL analysis with standard Python")
                status_suffix = " (standard Python)"
            
            bench = CtypesBenchmark()
            
            # Test both single-threaded and multi-threaded scenarios
            def cpu_intensive_ffi_task():
                """CPU-intensive FFI task for threading comparison."""
                total = 0
                for _ in range(1000):
                    total += bench.compute_operations_dot_product(100)
                return total
            
            # Single-threaded baseline
            print("  üìè Measuring single-threaded baseline...")
            single_thread_time = self.timer.measure_with_warmup(cpu_intensive_ffi_task)['wall_time']['median_ns']
            
            # Multi-threaded comparison
            thread_counts = [2, 4, 8] if gil_disabled else [2, 4]
            threading_results = {'single_thread': single_thread_time}
            
            for num_threads in thread_counts:
                print(f"  üßµ Testing with {num_threads} threads...")
                
                def run_parallel_ffi():
                    with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                        # Submit tasks
                        futures = [executor.submit(cpu_intensive_ffi_task) for _ in range(num_threads)]
                        # Wait for completion
                        results = [future.result() for future in concurrent.futures.as_completed(futures)]
                        return sum(results)
                
                # Measure multi-threaded performance
                multi_thread_time = self.timer.measure_with_warmup(run_parallel_ffi)['wall_time']['median_ns']
                threading_results[f'{num_threads}_threads'] = multi_thread_time
                
                # Calculate scaling efficiency
                ideal_time = single_thread_time / num_threads  # Perfect scaling
                actual_speedup = single_thread_time / multi_thread_time
                scaling_efficiency = actual_speedup / num_threads
                
                print(f"    üìä {num_threads} threads: {actual_speedup:.2f}x speedup, {scaling_efficiency:.1%} efficiency")
            
            # Analyze GIL impact
            if len(threading_results) > 2:  # Have multi-threaded results
                best_speedup = max([
                    single_thread_time / threading_results[key] 
                    for key in threading_results.keys() 
                    if key != 'single_thread'
                ])
                
                max_threads = max([
                    int(key.split('_')[0]) 
                    for key in threading_results.keys() 
                    if '_threads' in key
                ])
                
                max_efficiency = best_speedup / max_threads
                
                # Determine hypothesis status
                if gil_disabled:
                    # With GIL disabled, expect good scaling
                    significant_impact = max_efficiency < 0.7  # Less than 70% efficiency indicates GIL impact
                    status = HypothesisStatus.REJECTED if significant_impact else HypothesisStatus.SUPPORTED
                    conclusion = f"GIL disabled: {best_speedup:.2f}x max speedup, {max_efficiency:.1%} efficiency{status_suffix}"
                else:
                    # With GIL enabled, expect poor scaling (confirming GIL impact)
                    significant_impact = max_efficiency < 0.5  # Less than 50% efficiency expected with GIL
                    status = HypothesisStatus.SUPPORTED if significant_impact else HypothesisStatus.REJECTED
                    conclusion = f"GIL enabled: {best_speedup:.2f}x max speedup, {max_efficiency:.1%} efficiency confirms GIL impact{status_suffix}"
                
                self.results['H8'] = HypothesisResult(
                    hypothesis_id='H8',
                    description='GIL significantly impacts FFI performance in threaded scenarios',
                    status=status,
                    performance_data={
                        'gil_disabled': gil_disabled,
                        'threading_results': threading_results,
                        'best_speedup': best_speedup,
                        'max_efficiency': max_efficiency,
                        'thread_counts_tested': thread_counts
                    },
                    conclusion=conclusion,
                    confidence_level=0.85 if gil_disabled else 0.75,
                    effect_size=1.0 - max_efficiency,  # Impact magnitude
                    practical_significance=True
                )
            else:
                # Fallback for limited testing
                self.results['H8'] = HypothesisResult(
                    hypothesis_id='H8',
                    description='GIL significantly impacts FFI performance in threaded scenarios',
                    status=HypothesisStatus.INCONCLUSIVE,
                    performance_data={'gil_disabled': gil_disabled, 'threading_results': threading_results},
                    conclusion=f"Limited threading analysis{status_suffix}",
                    confidence_level=0.5
                )
            
        except Exception as e:
            self.results['H8'] = HypothesisResult(
                hypothesis_id='H8',
                description='GIL significantly impacts FFI performance in threaded scenarios',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during GIL analysis: {str(e)}"
            )
    
    def verify_h9_dict_vs_if_elif_scaling(self):
        """H9: Dictionary lookup outperforms if/elif chains for large function sets."""
        print("\nüß™ H9: Verifying dictionary vs if/elif scaling")
        
        try:
            results = self.dispatch_benchmark.test_hypothesis_h9()
            
            # Check if dictionary advantage increases with function count
            advantages = [r['dict_advantage'] for r in results.values()]
            scaling_trend = all(
                advantages[i+1] >= advantages[i] for i in range(len(advantages)-1)
            )
            
            # Dictionary should be faster for large function sets
            large_set_advantage = advantages[-1]  # 100 functions
            
            status = HypothesisStatus.SUPPORTED if (
                scaling_trend and large_set_advantage > 1.5
            ) else HypothesisStatus.REJECTED
            
            self.results['H9'] = HypothesisResult(
                hypothesis_id='H9',
                description='Dictionary lookup outperforms if/elif chains for large function sets',
                status=status,
                performance_data=results,
                conclusion=f"Dictionary {large_set_advantage:.1f}x faster for 100 functions",
                confidence_level=0.95,
                effect_size=large_set_advantage - 1.0,
                practical_significance=large_set_advantage > 1.5
            )
            
        except Exception as e:
            self.results['H9'] = HypothesisResult(
                hypothesis_id='H9',
                description='Dictionary vs if/elif scaling',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h10_cache_warming_effects(self):
        """H10: Cache warming significantly improves dynamic dispatch performance."""
        print("\nüß™ H10: Verifying cache warming effects")
        
        try:
            results = self.dispatch_benchmark.test_hypothesis_h10()
            
            speedup = results['cache_speedup']
            
            status = HypothesisStatus.SUPPORTED if speedup >= 2.0 else HypothesisStatus.REJECTED
            
            self.results['H10'] = HypothesisResult(
                hypothesis_id='H10',
                description='Cache warming significantly improves dynamic dispatch performance',
                status=status,
                performance_data=results,
                conclusion=f"Cache warming provides {speedup:.1f}x speedup",
                confidence_level=0.90,
                effect_size=speedup - 1.0,
                practical_significance=speedup >= 2.0
            )
            
        except Exception as e:
            self.results['H10'] = HypothesisResult(
                hypothesis_id='H10',
                description='Cache warming effects',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def verify_h11_locality_effects(self):
        """H11: Function call locality affects dispatch pattern efficiency."""
        print("\nüß™ H11: Verifying locality effects on dispatch")
        
        try:
            results = self.dispatch_benchmark.test_hypothesis_h11()
            
            # Compare sequential vs random access patterns
            sequential_perf = results.get('sequential', {}).get('ns_per_call', float('inf'))
            random_perf = results.get('random', {}).get('ns_per_call', float('inf'))
            hotspot_perf = results.get('hotspot_80_20', {}).get('ns_per_call', float('inf'))
            
            # Sequential should be fastest, random slowest
            locality_benefit = random_perf / sequential_perf
            hotspot_benefit = random_perf / hotspot_perf
            
            status = HypothesisStatus.SUPPORTED if (
                locality_benefit > 1.2 and hotspot_benefit > 1.1
            ) else HypothesisStatus.REJECTED
            
            self.results['H11'] = HypothesisResult(
                hypothesis_id='H11',
                description='Function call locality affects dispatch pattern efficiency',
                status=status,
                performance_data=results,
                conclusion=f"Sequential {locality_benefit:.1f}x faster than random",
                confidence_level=0.85,
                effect_size=locality_benefit - 1.0,
                practical_significance=locality_benefit > 1.2
            )
            
        except Exception as e:
            self.results['H11'] = HypothesisResult(
                hypothesis_id='H11',
                description='Locality effects on dispatch',
                status=HypothesisStatus.ERROR,
                conclusion=f"Error during verification: {str(e)}"
            )
    
    def _generate_final_report(self):
        """Generate comprehensive final report."""
        report_path = self.output_dir / "hypothesis_verification_report.md"
        
        with open(report_path, 'w') as f:
            f.write("# FFI Benchmark Hypothesis Verification Report\n\n")
            f.write(f"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # Summary statistics
            supported = sum(1 for r in self.results.values() if r.status == HypothesisStatus.SUPPORTED)
            rejected = sum(1 for r in self.results.values() if r.status == HypothesisStatus.REJECTED)
            inconclusive = sum(1 for r in self.results.values() if r.status == HypothesisStatus.INCONCLUSIVE)
            errors = sum(1 for r in self.results.values() if r.status == HypothesisStatus.ERROR)
            
            f.write("## Summary\n\n")
            f.write(f"- **Supported**: {supported}\n")
            f.write(f"- **Rejected**: {rejected}\n") 
            f.write(f"- **Inconclusive**: {inconclusive}\n")
            f.write(f"- **Errors**: {errors}\n\n")
            
            # Individual results
            f.write("## Detailed Results\n\n")
            
            for hypothesis_id in sorted(self.results.keys()):
                result = self.results[hypothesis_id]
                
                f.write(f"### {hypothesis_id}: {result.description}\n\n")
                f.write(f"**Status**: {result.status.value.upper()}\n\n")
                f.write(f"**Conclusion**: {result.conclusion}\n\n")
                
                if result.confidence_level > 0:
                    f.write(f"**Confidence Level**: {result.confidence_level:.2%}\n\n")
                
                if result.effect_size != 0:
                    f.write(f"**Effect Size**: {result.effect_size:.3f}\n\n")
                
                f.write(f"**Practical Significance**: {'Yes' if result.practical_significance else 'No'}\n\n")
                f.write("---\n\n")
        
        print(f"\nüìä Final report generated: {report_path}")
        
        # Post-verification environment check
        if self.environment_report:
            print(f"\nüå°Ô∏è Post-verification environment check...")
            post_report = self.environment_validator.validate_environment()
            
            # Compare environment stability
            score_change = post_report.validation_score - self.environment_report.validation_score
            if abs(score_change) > 0.1:
                print(f"‚ö†Ô∏è Environment score changed during verification: {score_change:+.2f}")
                if score_change < -0.2:
                    print("üö® Significant environment degradation detected!")
                    print("üìã Results may be affected by system instability")
            else:
                print(f"‚úÖ Environment remained stable (score: {post_report.validation_score:.2f})")
        
        print(f"\n‚úÖ Hypothesis verification completed!")
        print(f"   Supported: {supported}, Rejected: {rejected}, Inconclusive: {inconclusive}, Errors: {errors}")
        if self.environment_report:
            print(f"   Environment: {self.environment_report.status.value} (score: {self.environment_report.validation_score:.2f})")


def create_hypothesis_verification_framework(output_dir: str = None) -> HypothesisVerificationFramework:
    """Factory function to create hypothesis verification framework."""
    return HypothesisVerificationFramework(output_dir)


if __name__ == "__main__":
    # Run hypothesis verification
    framework = create_hypothesis_verification_framework()
    results = framework.verify_all_hypotheses()
    
    print("\nüéØ Hypothesis Verification Summary:")
    print("=" * 50)
    
    for hypothesis_id, result in results.items():
        status_icon = {
            HypothesisStatus.SUPPORTED: "‚úÖ",
            HypothesisStatus.REJECTED: "‚ùå", 
            HypothesisStatus.INCONCLUSIVE: "‚ö†Ô∏è",
            HypothesisStatus.ERROR: "üö®"
        }[result.status]
        
        print(f"{status_icon} {hypothesis_id}: {result.conclusion}")